import Anthropic from "@anthropic-ai/sdk";
import 'dotenv/config';
import enrichLeadContext from "../services/leadContext.js"; // â† IMPORTA, nÃ£o define
import { getManual } from './amandaIntents.js';
import { detectAllFlags } from './flagsDetector.js';
import { buildEquivalenceResponse } from './responseBuilder.js';
import {
    detectAllTherapies,
    getTDAHResponse,
    isAskingAboutEquivalence,
    isTDAHQuestion
} from './therapyDetector.js';

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

/**
 * ðŸŽ¯ ORQUESTRADOR COM CONTEXTO INTELIGENTE
 */
export async function getOptimizedAmandaResponse({ content, userText, lead = {}, context = {} }) {
    const text = userText || content || "";
    const normalized = text.toLowerCase().trim();

    console.log(`ðŸŽ¯ [ORCHESTRATOR] Processando: "${text}"`);

    // âœ… CONTEXTO INTELIGENTE (busca de leadContext.js)
    const enrichedContext = lead._id
        ? await enrichLeadContext(lead._id)
        : { stage: 'novo', isFirstContact: true, messageCount: 0, conversationHistory: [], conversationSummary: null, shouldGreet: true };

    // ===== 1. TDAH - RESPOSTA ESPECÃFICA =====
    if (isTDAHQuestion(text)) {
        console.log('ðŸ§  [TDAH] Pergunta sobre tratamento TDAH detectada');
        return getTDAHResponse(lead?.name);
    }

    // ===== 2. TERAPIAS ESPECÃFICAS =====
    const therapies = detectAllTherapies(text);

    if (therapies.length > 0) {
        console.log(`ðŸŽ¯ [TERAPIAS] Detectadas: ${therapies.map(t => t.id).join(', ')}`);

        const flags = detectAllFlags(text, lead, enrichedContext);

        console.log(`ðŸ [FLAGS]`, {
            asksPrice: flags.asksPrice,
            wantsSchedule: flags.wantsSchedule,
            userProfile: flags.userProfile
        });

        const aiResponse = await callClaudeWithTherapyData({
            therapies,
            flags,
            userText: text,
            lead,
            context: enrichedContext
        });

        return ensureSingleHeart(aiResponse);
    }

    // ===== 3. EQUIVALÃŠNCIA =====
    if (isAskingAboutEquivalence(text)) {
        return buildEquivalenceResponse();
    }

    // ===== 4. MANUAL =====
    const manualResponse = tryManualResponse(normalized);
    if (manualResponse) {
        console.log(`âœ… [ORCHESTRATOR] Resposta do manual`);
        return ensureSingleHeart(manualResponse);
    }

    // ===== 5. IA COM CONTEXTO =====
    console.log(`ðŸ¤– [ORCHESTRATOR] IA | Stage: ${enrichedContext.stage} | Msgs: ${enrichedContext.messageCount}`);
    try {
        const aiResponse = await callOpenAIWithContext(text, lead, enrichedContext);
        return ensureSingleHeart(aiResponse);
    } catch (error) {
        console.error(`âŒ [ORCHESTRATOR] Erro na IA:`, error.message);
        return "Vou verificar e jÃ¡ te retorno, por favor um momento ðŸ’š";
    }
}

/**
 * ðŸ“– MANUAL
 */
function tryManualResponse(normalizedText) {
    if (/\b(endere[cÃ§]o|onde fica|local|mapa|como chegar)\b/.test(normalizedText)) {
        return getManual('localizacao', 'endereco');
    }

    if (/\b(plano|conv[eÃª]nio|unimed|ipasgo|amil)\b/.test(normalizedText)) {
        return getManual('planos_saude', 'unimed');
    }

    if (/\b(pre[cÃ§]o|valor|quanto.*custa)\b/.test(normalizedText) &&
        !/\b(neuropsic|fono|psico|terapia|fisio|musico)\b/.test(normalizedText)) {
        return getManual('valores', 'consulta');
    }

    if (/^(oi|ol[aÃ¡]|boa\s*(tarde|noite|dia)|bom\s*dia)[\s!,.]*$/i.test(normalizedText)) {
        return getManual('saudacao');
    }

    return null;
}

/**
 * ðŸ¤– IA COM DADOS DE TERAPIAS + HISTÃ“RICO COMPLETO + CACHE MÃXIMO
 */
async function callClaudeWithTherapyData({ therapies, flags, userText, lead, context }) {
    const { getTherapyData } = await import('./therapyDetector.js');
    const { getLatestInsights } = await import('../services/amandaLearningService.js');
    const { SYSTEM_PROMPT_AMANDA } = await import('./amandaPrompt.js');

    const insights = await getLatestInsights();

    const therapiesInfo = therapies.map(t => {
        const data = getTherapyData(t.id);
        return `${t.name.toUpperCase()}: ${data.explanation} | PreÃ§o: ${data.price}`;
    }).join('\n');

    const {
        stage, messageCount, isPatient, hasAppointments,
        needsUrgency, daysSinceLastContact,
        conversationHistory, conversationSummary, shouldGreet
    } = context;

    // âœ… INSIGHTS APRENDIDOS
    let learnedContext = '';
    if (insights?.data?.effectivePriceResponses && flags.asksPrice) {
        const scenario = stage === 'novo' ? 'first_contact' : 'engaged';
        const bestResponse = insights.data.effectivePriceResponses.find(r => r.scenario === scenario);
        if (bestResponse) {
            learnedContext = `\nðŸ’¡ PADRÃƒO DE SUCESSO: "${bestResponse.response}"`;
        }
    }

    const patientStatus = isPatient ? `\nâš ï¸ PACIENTE ATIVO - Tom prÃ³ximo!` : '';
    const urgencyNote = needsUrgency ? `\nðŸ”¥ ${daysSinceLastContact} dias sem falar - reative com calor!` : '';

    // ðŸ§  PREPARA PROMPT ATUAL
    const currentPrompt = `${userText}

ðŸ“Š CONTEXTO DESTA MENSAGEM:
TERAPIAS DETECTADAS: ${therapiesInfo}
FLAGS: PreÃ§o=${flags.asksPrice} | Agendar=${flags.wantsSchedule}
ESTÃGIO: ${stage} (${messageCount} msgs totais)${patientStatus}${urgencyNote}${learnedContext}

ðŸŽ¯ INSTRUÃ‡Ã•ES CRÃTICAS:
1. ${shouldGreet ? 'âœ… Pode cumprimentar naturalmente' : 'ðŸš¨ NÃƒO USE SAUDAÃ‡Ã•ES (Oi/OlÃ¡) - conversa estÃ¡ ativa'}
2. ${conversationSummary ? 'ðŸ§  VocÃª TEM o resumo completo acima - USE esse contexto!' : 'ðŸ“œ Leia TODO o histÃ³rico de mensagens acima'}
3. ðŸš¨ NÃƒO PERGUNTE o que JÃ foi informado/discutido
4. ${flags.asksPrice ? 'Responda preÃ§o: VALORâ†’PREÃ‡Oâ†’PERGUNTA' : 'Apresente de forma acolhedora'}
5. MÃ¡ximo 3 frases, tom natural e humano
6. Exatamente 1 ðŸ’š no final`;

    // ðŸ§  MONTA MENSAGENS COM CACHE MÃXIMO
    const messages = [];

    // 1. Resumo com cache
    if (conversationSummary) {
        messages.push({
            role: 'user',
            content: `ðŸ“‹ CONTEXTO DE CONVERSAS ANTERIORES:\n\n${conversationSummary}\n\n---\n\nAs mensagens abaixo sÃ£o a continuaÃ§Ã£o RECENTE desta conversa:`,
            cache_control: { type: "ephemeral" }
        });
        messages.push({
            role: 'assistant',
            content: 'Entendi o contexto completo. Vou continuar a conversa de forma natural, lembrando de tudo que foi discutido.'
        });
    }

    // 2. HistÃ³rico com cache na Ãºltima mensagem
    if (conversationHistory.length > 0) {
        messages.push(...conversationHistory.slice(0, -1));

        const lastHistoryMsg = conversationHistory[conversationHistory.length - 1];
        messages.push({
            ...lastHistoryMsg,
            cache_control: { type: "ephemeral" }
        });
    }

    // 3. Mensagem atual (SEM cache)
    messages.push({
        role: 'user',
        content: currentPrompt
    });

    // ðŸš€ CHAMA ANTHROPIC COM CACHE
    const response = await anthropic.messages.create({
        model: "claude-sonnet-4-20250514",
        max_tokens: 200,
        temperature: 0.7,
        system: [
            {
                type: "text",
                text: SYSTEM_PROMPT_AMANDA,
                cache_control: { type: "ephemeral" }
            }
        ],
        messages
    });

    return response.content[0]?.text?.trim() || "Como posso te ajudar? ðŸ’š";
}

/**
 * ðŸ¤– IA COM CONTEXTO INTELIGENTE + CACHE MÃXIMO
 */
async function callOpenAIWithContext(userText, lead, context) {
    const { SYSTEM_PROMPT_AMANDA } = await import('./amandaPrompt.js');

    const {
        stage = 'novo',
        messageCount = 0,
        mentionedTherapies = [],
        isPatient = false,
        needsUrgency = false,
        daysSinceLastContact = 0,
        conversationHistory = [],
        conversationSummary = null,
        shouldGreet = true
    } = context;

    let stageInstruction = '';
    switch (stage) {
        case 'novo':
            stageInstruction = 'Seja acolhedora. Pergunte necessidade antes de preÃ§os.';
            break;
        case 'pesquisando_preco':
            stageInstruction = 'Lead jÃ¡ perguntou valores. Use VALORâ†’PREÃ‡Oâ†’ENGAJAMENTO.';
            break;
        case 'engajado':
            stageInstruction = `Lead trocou ${messageCount} msgs. Seja mais direta.`;
            break;
        case 'interessado_agendamento':
            stageInstruction = 'Lead quer agendar! OfereÃ§a 2 perÃ­odos concretos.';
            break;
        case 'paciente':
            stageInstruction = 'PACIENTE ATIVO! Tom prÃ³ximo.';
            break;
    }

    const patientNote = isPatient ? `\nâš ï¸ PACIENTE - seja prÃ³xima!` : '';
    const urgencyNote = needsUrgency ? `\nðŸ”¥ ${daysSinceLastContact} dias sem contato - reative!` : '';
    const therapiesContext = mentionedTherapies.length > 0
        ? `\nðŸŽ¯ TERAPIAS DISCUTIDAS: ${mentionedTherapies.join(', ')}`
        : '';

    // ðŸ§  PREPARA PROMPT ATUAL
    const currentPrompt = `${userText}

CONTEXTO:
LEAD: ${lead?.name || 'Desconhecido'} | ESTÃGIO: ${stage} (${messageCount} msgs)${therapiesContext}${patientNote}${urgencyNote}

INSTRUÃ‡ÃƒO: ${stageInstruction}

REGRAS:
- ${shouldGreet ? 'Pode cumprimentar' : 'ðŸš¨ NÃƒO use Oi/OlÃ¡ - conversa ativa'}
- ${conversationSummary ? 'ðŸ§  USE o resumo acima' : 'ðŸ“œ Leia histÃ³rico acima'}
- ðŸš¨ NÃƒO pergunte o que jÃ¡ foi dito
- 1-3 frases, tom humano
- 1 pergunta engajadora
- 1 ðŸ’š final`;

    // ðŸ§  MONTA MENSAGENS COM CACHE MÃXIMO
    const messages = [];

    // 1. Resumo com cache
    if (conversationSummary) {
        messages.push({
            role: 'user',
            content: `ðŸ“‹ CONTEXTO ANTERIOR:\n\n${conversationSummary}\n\n---\n\nMensagens recentes abaixo:`,
            cache_control: { type: "ephemeral" }
        });
        messages.push({
            role: 'assistant',
            content: 'Entendi o contexto. Continuando...'
        });
    }

    // 2. HistÃ³rico com cache na Ãºltima mensagem
    if (conversationHistory.length > 0) {
        messages.push(...conversationHistory.slice(0, -1));

        const lastHistoryMsg = conversationHistory[conversationHistory.length - 1];
        messages.push({
            ...lastHistoryMsg,
            cache_control: { type: "ephemeral" }
        });
    }

    // 3. Mensagem atual (SEM cache)
    messages.push({
        role: 'user',
        content: currentPrompt
    });

    const response = await anthropic.messages.create({
        model: "claude-sonnet-4-20250514",
        max_tokens: 150,
        temperature: 0.6,
        system: [
            {
                type: "text",
                text: SYSTEM_PROMPT_AMANDA,
                cache_control: { type: "ephemeral" }
            }
        ],
        messages
    });

    return response.content[0]?.text?.trim() || "Como posso te ajudar? ðŸ’š";
}

/**
 * ðŸŽ¨ HELPER
 */
function ensureSingleHeart(text) {
    if (!text) return "Como posso te ajudar? ðŸ’š";
    const clean = text.replace(/ðŸ’š/g, '').trim();
    return `${clean} ðŸ’š`;
}

export default getOptimizedAmandaResponse;